{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Árboles de Decisión\n",
    "\n",
    "Veremos árboles de decisión y los conceptos subyacentes asociados.\n",
    "\n",
    "Haremos ejemplos de juguete y con datos generados artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropía y Ganancia de Información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía\n",
    "\n",
    "Definamos entropía para una distribución probabilista:\n",
    "\n",
    "$$H(Y) = - \\sum_{i=1}^k P(Y = y_i) log_2 P(Y = y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probs):\n",
    "    return - np.sum(probs * np.log2(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos posibles entropías para el problema de tirar una moneda adulterada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.01, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1)[1:-1]\n",
    "plt.plot(X, [entropy([x, 1-x]) for x in X])\n",
    "plt.xlabel('P(Y=y_1)')\n",
    "plt.ylabel('entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de dos monedas, tenemos cuatro combinaciones posibles, siendo $0.25$ la probabilidad de cada evento. Ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.25, 0.25, 0.25, 0.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero si las monedas estan sesgadas tendríamos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.49, 0.49, 0.01, 0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía de un Dataset\n",
    "\n",
    "Un dataset define una distribución empírica. La entropía del dataset es entones la entropía de la distribución asociada. Definamos el cálculo de la distribución, y luego redefinamos entropía:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un dataset de 6 elementos con dos atributos $X_1$ y $X_2$ con dos tipos de etiquetas: \n",
    "\n",
    "$False: -1$ y $True: 1$. \n",
    "<img src=\"img/tablaDataset2.png\" width=\"100\"/>\n",
    "Siendo las etiquetas resultantes del mismo: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 1, 1, 1, 1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la función \"probs\" podemos calcular la probilidad de cada clase en este dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esos resultados nos informan que hay un desbalance en el mismo (hay más probabilidad de una de las clases).\n",
    "\n",
    "Ahora obtengamos la entropía del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    p = probs(y)\n",
    "    return - np.sum(p * np.log2(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía Condicional\n",
    "\n",
    "Definamos entropía condicional:\n",
    "\n",
    "$$H(Y|X) = - \\sum_{j=1}^v P(X = x_j) \\sum_{i=1}^k P(Y = y_i | X = x_j) log_2 P(Y = y_i | X = x_j)$$\n",
    "\n",
    "Equivalentemente, \n",
    "\n",
    "$$H(Y|X) = \\sum_{j=1}^v P(X = x_j) H(Y|X = x_j)$$\n",
    "\n",
    "Tomaremos $X$ binaria ($v=2$), por lo que la entropía condicional tendrá sólo dos términos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_entropy(y1, y2):\n",
    "    size = y1.shape[0] + y2.shape[0]\n",
    "    return y1.shape[0] / size * entropy(y1) + y2.shape[0] / size * entropy(y2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función toma como argumento los dos subconjuntos de datos que se forman al considerar una de las variables como condición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizámos la varibale $Y$ donde la $X_1$ es $True$, tenemos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_X1true = np.array([1,1,1,1]) #X1 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de igual manera, la varibale $Y$  donde la $X_1$ es $False$, tenemos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_X1false =  np.array([1,-1]) #X1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de la función \"cond_entropy\" podemos obtener la entropía condicional de la variable $Y$ dado $X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_entropy(y_X1true,y_X1false)  # x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera podemos calcular la entropía condicional dado la variable $X_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_X2true = np.array([1,1,1]) #X2 = True\n",
    "y_X2false = np.array([1,1,-1]) #X2 = False\n",
    "cond_entropy(y_X2true, y_X2false)  # x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ganancia de Información\n",
    "\n",
    "La ganancia de información será simplemente la diferencia entre entropía y entropía condicional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y1, y2):\n",
    "    y = np.concatenate((y1,y2))\n",
    "    return entropy(y) - cond_entropy(y1,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular la ganancia de información que implicaría dividir el dataset tomando como $X_1$ como nodo referencia. Para ello hacemos uso de la función \"information_gain\" que toma los dos subconjuntos de etiquetas que se formarían al tomar una u otra variable como referencia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de $X_1$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(y_X1true, y_X1false)  # x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de $X_2$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(y_X2true, y_X2false)  # x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Sintéticos No Linealmente Separables\n",
    "\n",
    "Haremos algunos experimentos con datos generados sintéticamente. Estos datos serán no linealmente separables.\n",
    "\n",
    "Ejemplos típicos de datos no linealmente separables son los de tipo \"OR\", \"AND\" y \"XOR\". Usaremos datos de tipo \"OR\" para este ejemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(size, 2)\n",
    "y_true = np.logical_or(X[:, 0] > 0, X[:, 1] > 0)    # datos \"OR\"\n",
    "#y_true = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)  # datos \"XOR\"\n",
    "#y_true = np.logical_and(X[:, 0] > 0, X[:, 1] > 0)  # datos \"AND\"\n",
    "y_true = y_true.astype(int)\n",
    "y_true[y_true == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y_true==1, 0], X[y_true==1, 1], color=\"royalblue\", label=\"1\")\n",
    "plt.scatter(X[y_true==-1, 0], X[y_true==-1, 1], color=\"red\", label=\"-1\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"X[0]\")\n",
    "plt.ylabel(\"X[1]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en Entrenamiento y Evaluación\n",
    "\n",
    "Separemos la mitad para entrenamiento y la otra para evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50\n",
    "test_size = size - train_size\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_true[:train_size], y_true[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación Lineal\n",
    "\n",
    "Veamos qué tan mal anda un clasificador lineal sobre estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary\n",
    "plt.xlabel(\"X[0]\")\n",
    "plt.ylabel(\"X[1]\")\n",
    "plot_decision_boundary(lambda x: model.predict(x), X, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos la calidad de la predicción en entrenamiento y evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Train accuracy: {train_acc:0.2}')\n",
    "print(f'Test accuracy: {test_acc:0.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota al Margen: Induciendo Separabilidad Lineal\n",
    "\n",
    "Muchas veces se pueden convertir datos no linealmente separables en datos separables (o casi) mediante la introducción de nuevos atributos que combinan los atributos existentes.\n",
    "Un ejemplo de estos son los atributos polinomiales.\n",
    "\n",
    "Aquí lo haremos con datos \"OR\", pero la diferencia es mucho más notable con datos de tipo \"XOR\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pre = PolynomialFeatures(\n",
    "    degree=2,\n",
    "    interaction_only=True,  # para usar solo x0*x1, no x0*x0 ni x1*x1\n",
    "    include_bias=False)\n",
    "X_train2 = pre.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_train2.shape  # se agregó el feature x0*x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train2[y_train==1, 1], X_train2[y_train==1, 2], color=\"dodgerblue\", edgecolors='k', label=\"1\")\n",
    "plt.scatter(X_train2[y_train==-1, 1], X_train2[y_train==-1, 2], color=\"tomato\", edgecolors='k', label=\"-1\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "    LogisticRegression()\n",
    ")\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Train accuracy: {train_acc:0.2}')\n",
    "print(f'Test accuracy: {test_acc:0.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary\n",
    "\n",
    "plot_decision_boundary(lambda x: model.predict(x), X, y_true)\n",
    "plt.xlabel(\"X[0]\")\n",
    "plt.ylabel(\"X[1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía y Valores Reales\n",
    "\n",
    "Calculemos la entropía inicial, y veamos cómo condicionar la entropía sobre variables reales (i.e. no categóricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular la probabilidad de cada clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y la entropía:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una división sobre una variable real usaremos un valor \"threshold\" (umbral):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, i, threshold):\n",
    "    y1 = y[X[:, i] > threshold]\n",
    "    y2 = y[X[:, i] <= threshold]\n",
    "    return y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definimos la \"split\" que toma como argumento el dataset con sus etiquetas, el indice de la variable que usaremos para dividir el dataset y el umbral para realizarlo y nos devuelve dos subconjuntos de este dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizamos la variable $X_1$ con un umbral de $0.00$ obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = split(X_train, y_train, 0, 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1,y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siendo la entropía de cada uno de ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y1), entropy(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_entropy(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(y1,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando la Mejor División\n",
    "\n",
    "Ilustraremos un paso en la construcción del árbol de decisión.\n",
    "\n",
    "Probemos muchos threshold para ambas variables y seleccionemos la mejor división.\n",
    "\n",
    "En este caso buscaremos en una grilla uniforme de valores, pero existen técnicas mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(-2.5, 2.5, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ig = 0\n",
    "\n",
    "for i in [0, 1]:\n",
    "    for threshold in np.linspace(-2.5, 2.5, 11):\n",
    "        y1, y2 = split(X_train, y_train, i, threshold)\n",
    "        ig = information_gain(y1, y2)\n",
    "        print(f'i={i}\\tthreshold={threshold:+00.2f}\\tig={ig:.2f}')\n",
    "\n",
    "        if ig >= best_ig:\n",
    "            best_ig = ig\n",
    "            best_feature = i\n",
    "            best_threshold = threshold\n",
    "\n",
    "print('Mejor división:')\n",
    "print(f'feature={best_feature}, threshold={best_threshold}, ig={best_ig:00.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividamos los datos de acuerdo a esta frontera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los datos en el gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], color=\"royalblue\", label=\"1\")\n",
    "plt.scatter(X_train[y_train==-1, 0], X_train[y_train==-1, 1], color=\"red\", label=\"-1\")\n",
    "\n",
    "plt.axhline(y=0, xmin=-3, xmax=3, color='green', linestyle='-.', linewidth=2)\n",
    "\n",
    "plt.xlabel(\"X[0]\")\n",
    "plt.ylabel(\"X[1]\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = split(X_train, y_train, best_feature, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta división, la entropía baja considerablemente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_entropy(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de Decisión con Scikit-learn\n",
    "\n",
    "Aprendamos un árbol de decisión usando scikit-learn. Para ello usaremos la clase [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#DecisionTreeClassifier?\n",
    "clf = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=0)\n",
    "#clf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora predecimos y evaluamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Train accuracy: {train_acc:0.2}')\n",
    "print(f'Test accuracy: {test_acc:0.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujamos la frontera de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary\n",
    "\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X_train, y_train)\n",
    "plt.xlabel(\"X[0]\")\n",
    "plt.ylabel(\"X[1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos inspeccionar el árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(clf,filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Probar todos los experimentos con un dataset de tipo \"XOR\". ¿Qué sucede al decidir la división en el primer nivel del árbol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "Scikit-learn:\n",
    "\n",
    "- [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "- [User Guide: Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
